2025-04-15 01:08:37,892 transreid INFO: Saving model in the path :./logs/mpdd
2025-04-15 01:08:37,892 transreid INFO: Namespace(config_file='configs/MPDD/vit_clipreid.yml', opts=[], local_rank=0)
2025-04-15 01:08:37,892 transreid INFO: Loaded configuration file configs/MPDD/vit_clipreid.yml
2025-04-15 01:08:37,894 transreid INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'ViT-B-16'
  STRIDE_SIZE: [16, 16]
  ID_LOSS_WEIGHT : 0.25
  TRIPLET_LOSS_WEIGHT : 1.0
  I2T_LOSS_WEIGHT : 1.0
  SIE_CAMERA: False
  SIE_COE : 1.0

INPUT:
  SIZE_TRAIN: [256, 128]
  SIZE_TEST: [256, 128]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.5 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  STAGE1:
    IMS_PER_BATCH: 1
    OPTIMIZER_NAME: "Adam"
    BASE_LR: 0.00055
    WARMUP_LR_INIT: 0.00001
    LR_MIN: 1e-6
    WARMUP_METHOD: 'linear'
    WEIGHT_DECAY:  1e-4
    WEIGHT_DECAY_BIAS: 1e-4
    MAX_EPOCHS: 40
    CHECKPOINT_PERIOD: 40
    LOG_PERIOD: 340
    WARMUP_EPOCHS: 5
  
  STAGE2:
    IMS_PER_BATCH: 32
    OPTIMIZER_NAME: "Adam"
    BASE_LR: 0.000008
    WARMUP_METHOD: 'linear'
    WARMUP_ITERS: 10
    WARMUP_FACTOR: 0.1
    WEIGHT_DECAY:  0.0001
    WEIGHT_DECAY_BIAS: 0.0001
    LARGE_FC_LR: False
    MAX_EPOCHS: 30
    CHECKPOINT_PERIOD: 30
    LOG_PERIOD: 10
    EVAL_PERIOD: 30
    BIAS_LR_FACTOR: 2
    
    STEPS: [30, 50]
    GAMMA: 0.1
  
TEST:
  EVAL: False
  IMS_PER_BATCH: 64
  RE_RANKING: False
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'



DATASETS:
   NAMES: ('mpdd')
   ROOT_DIR: ('./data')
OUTPUT_DIR: './logs/mpdd'




# CUDA_VISIBLE_DEVICES=0 python train_clipreid.py --config_file configs/MPDD/vit_clipreid.yml
2025-04-15 01:08:37,895 transreid INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: mpdd
  ROOT_DIR: ./data
INPUT:
  PADDING: 0
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
MODEL:
  ATT_DROP_RATE: 0.0
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  I2T_LOSS_WEIGHT: 1.0
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  METRIC_LOSS_TYPE: triplet
  NAME: ViT-B-16
  NECK: bnneck
  NO_MARGIN: False
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: 
  SIE_CAMERA: False
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: None
  TRIPLET_LOSS_WEIGHT: 1.0
OUTPUT_DIR: ./logs/mpdd
SOLVER:
  MARGIN: 0.3
  SEED: 451
  STAGE1:
    BASE_LR: 0.00055
    CHECKPOINT_PERIOD: 40
    COSINE_MARGIN: 0.5
    COSINE_SCALE: 30
    EVAL_PERIOD: 10
    IMS_PER_BATCH: 1
    LOG_PERIOD: 340
    LR_MIN: 1e-06
    MAX_EPOCHS: 40
    MOMENTUM: 0.9
    OPTIMIZER_NAME: Adam
    WARMUP_EPOCHS: 5
    WARMUP_FACTOR: 0.01
    WARMUP_ITERS: 500
    WARMUP_LR_INIT: 1e-05
    WARMUP_METHOD: linear
    WEIGHT_DECAY: 0.0001
    WEIGHT_DECAY_BIAS: 0.0001
  STAGE2:
    BASE_LR: 8e-06
    BIAS_LR_FACTOR: 2
    CENTER_LOSS_WEIGHT: 0.0005
    CENTER_LR: 0.5
    CHECKPOINT_PERIOD: 30
    COSINE_MARGIN: 0.5
    COSINE_SCALE: 30
    EVAL_PERIOD: 30
    GAMMA: 0.1
    IMS_PER_BATCH: 32
    LARGE_FC_LR: False
    LOG_PERIOD: 10
    LR_MIN: 1.6e-05
    MAX_EPOCHS: 30
    MOMENTUM: 0.9
    OPTIMIZER_NAME: Adam
    STEPS: (30, 50)
    WARMUP_EPOCHS: 5
    WARMUP_FACTOR: 0.1
    WARMUP_ITERS: 10
    WARMUP_LR_INIT: 0.01
    WARMUP_METHOD: linear
    WEIGHT_DECAY: 0.0001
    WEIGHT_DECAY_BIAS: 0.0001
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: False
  FEAT_NORM: yes
  IMS_PER_BATCH: 64
  NECK_FEAT: before
  RE_RANKING: False
  WEIGHT: 
2025-04-15 01:08:39,877 transreid.train INFO: start training
2025-04-15 01:08:39,880 transreid.train INFO: model: build_transformer(
  (classifier): Linear(in_features=768, out_features=101, bias=False)
  (classifier_proj): Linear(in_features=512, out_features=101, bias=False)
  (bottleneck): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bottleneck_proj): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (prompt_learner): PromptLearner(
    (meta_net): Sequential(
      (linear1): Linear(in_features=512, out_features=32, bias=True)
      (relu): ReLU(inplace=True)
      (linear2): Linear(in_features=32, out_features=512, bias=True)
    )
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
2025-04-15 01:08:55,598 transreid.train INFO: Epoch[1] Iteration[340/945] Loss: 2.091, Base Lr: 1.18e-04
2025-04-15 01:09:02,414 transreid.train INFO: Epoch[1] Iteration[680/945] Loss: 1.839, Base Lr: 1.18e-04
2025-04-15 01:09:14,483 transreid.train INFO: Epoch[2] Iteration[340/945] Loss: 1.393, Base Lr: 2.26e-04
2025-04-15 01:09:21,227 transreid.train INFO: Epoch[2] Iteration[680/945] Loss: 1.360, Base Lr: 2.26e-04
2025-04-15 01:09:33,441 transreid.train INFO: Epoch[3] Iteration[340/945] Loss: 1.269, Base Lr: 3.34e-04
2025-04-15 01:09:40,314 transreid.train INFO: Epoch[3] Iteration[680/945] Loss: 1.256, Base Lr: 3.34e-04
2025-04-15 01:09:52,164 transreid.train INFO: Epoch[4] Iteration[340/945] Loss: 1.212, Base Lr: 4.42e-04
2025-04-15 01:09:58,822 transreid.train INFO: Epoch[4] Iteration[680/945] Loss: 1.208, Base Lr: 4.42e-04
2025-04-15 01:10:11,453 transreid.train INFO: Epoch[5] Iteration[340/945] Loss: 1.188, Base Lr: 5.29e-04
2025-04-15 01:10:18,208 transreid.train INFO: Epoch[5] Iteration[680/945] Loss: 1.184, Base Lr: 5.29e-04
2025-04-15 01:10:30,477 transreid.train INFO: Epoch[6] Iteration[340/945] Loss: 1.170, Base Lr: 5.20e-04
2025-04-15 01:10:37,449 transreid.train INFO: Epoch[6] Iteration[680/945] Loss: 1.169, Base Lr: 5.20e-04
2025-04-15 01:10:49,442 transreid.train INFO: Epoch[7] Iteration[340/945] Loss: 1.158, Base Lr: 5.10e-04
2025-04-15 01:10:56,121 transreid.train INFO: Epoch[7] Iteration[680/945] Loss: 1.158, Base Lr: 5.10e-04
2025-04-15 01:11:08,215 transreid.train INFO: Epoch[8] Iteration[340/945] Loss: 1.145, Base Lr: 4.98e-04
2025-04-15 01:11:14,960 transreid.train INFO: Epoch[8] Iteration[680/945] Loss: 1.144, Base Lr: 4.98e-04
2025-04-15 01:11:27,068 transreid.train INFO: Epoch[9] Iteration[340/945] Loss: 1.134, Base Lr: 4.84e-04
2025-04-15 01:11:33,932 transreid.train INFO: Epoch[9] Iteration[680/945] Loss: 1.135, Base Lr: 4.84e-04
2025-04-15 01:11:46,330 transreid.train INFO: Epoch[10] Iteration[340/945] Loss: 1.128, Base Lr: 4.70e-04
2025-04-15 01:11:53,404 transreid.train INFO: Epoch[10] Iteration[680/945] Loss: 1.124, Base Lr: 4.70e-04
2025-04-15 01:12:05,490 transreid.train INFO: Epoch[11] Iteration[340/945] Loss: 1.121, Base Lr: 4.54e-04
2025-04-15 01:12:12,219 transreid.train INFO: Epoch[11] Iteration[680/945] Loss: 1.120, Base Lr: 4.54e-04
2025-04-15 01:12:24,582 transreid.train INFO: Epoch[12] Iteration[340/945] Loss: 1.112, Base Lr: 4.37e-04
2025-04-15 01:12:31,125 transreid.train INFO: Epoch[12] Iteration[680/945] Loss: 1.111, Base Lr: 4.37e-04
2025-04-15 01:12:43,917 transreid.train INFO: Epoch[13] Iteration[340/945] Loss: 1.110, Base Lr: 4.19e-04
2025-04-15 01:12:50,999 transreid.train INFO: Epoch[13] Iteration[680/945] Loss: 1.110, Base Lr: 4.19e-04
2025-04-15 01:13:03,029 transreid.train INFO: Epoch[14] Iteration[340/945] Loss: 1.103, Base Lr: 4.00e-04
2025-04-15 01:13:09,922 transreid.train INFO: Epoch[14] Iteration[680/945] Loss: 1.103, Base Lr: 4.00e-04
2025-04-15 01:13:22,193 transreid.train INFO: Epoch[15] Iteration[340/945] Loss: 1.099, Base Lr: 3.81e-04
2025-04-15 01:13:28,827 transreid.train INFO: Epoch[15] Iteration[680/945] Loss: 1.099, Base Lr: 3.81e-04
2025-04-15 01:13:41,075 transreid.train INFO: Epoch[16] Iteration[340/945] Loss: 1.095, Base Lr: 3.60e-04
2025-04-15 01:13:47,671 transreid.train INFO: Epoch[16] Iteration[680/945] Loss: 1.095, Base Lr: 3.60e-04
2025-04-15 01:13:59,591 transreid.train INFO: Epoch[17] Iteration[340/945] Loss: 1.085, Base Lr: 3.40e-04
2025-04-15 01:14:06,347 transreid.train INFO: Epoch[17] Iteration[680/945] Loss: 1.090, Base Lr: 3.40e-04
2025-04-15 01:14:18,776 transreid.train INFO: Epoch[18] Iteration[340/945] Loss: 1.086, Base Lr: 3.18e-04
2025-04-15 01:14:25,655 transreid.train INFO: Epoch[18] Iteration[680/945] Loss: 1.086, Base Lr: 3.18e-04
2025-04-15 01:14:37,855 transreid.train INFO: Epoch[19] Iteration[340/945] Loss: 1.082, Base Lr: 2.97e-04
2025-04-15 01:14:44,829 transreid.train INFO: Epoch[19] Iteration[680/945] Loss: 1.085, Base Lr: 2.97e-04
2025-04-15 01:14:57,163 transreid.train INFO: Epoch[20] Iteration[340/945] Loss: 1.078, Base Lr: 2.76e-04
2025-04-15 01:15:04,008 transreid.train INFO: Epoch[20] Iteration[680/945] Loss: 1.079, Base Lr: 2.76e-04
2025-04-15 01:15:16,111 transreid.train INFO: Epoch[21] Iteration[340/945] Loss: 1.077, Base Lr: 2.54e-04
2025-04-15 01:15:22,811 transreid.train INFO: Epoch[21] Iteration[680/945] Loss: 1.076, Base Lr: 2.54e-04
2025-04-15 01:15:34,843 transreid.train INFO: Epoch[22] Iteration[340/945] Loss: 1.074, Base Lr: 2.33e-04
2025-04-15 01:15:41,497 transreid.train INFO: Epoch[22] Iteration[680/945] Loss: 1.075, Base Lr: 2.33e-04
2025-04-15 01:15:53,454 transreid.train INFO: Epoch[23] Iteration[340/945] Loss: 1.073, Base Lr: 2.11e-04
2025-04-15 01:16:00,052 transreid.train INFO: Epoch[23] Iteration[680/945] Loss: 1.071, Base Lr: 2.11e-04
2025-04-15 01:16:12,323 transreid.train INFO: Epoch[24] Iteration[340/945] Loss: 1.066, Base Lr: 1.91e-04
2025-04-15 01:16:19,441 transreid.train INFO: Epoch[24] Iteration[680/945] Loss: 1.066, Base Lr: 1.91e-04
2025-04-15 01:16:31,038 transreid.train INFO: Epoch[25] Iteration[340/945] Loss: 1.065, Base Lr: 1.70e-04
2025-04-15 01:16:37,799 transreid.train INFO: Epoch[25] Iteration[680/945] Loss: 1.064, Base Lr: 1.70e-04
2025-04-15 01:16:49,691 transreid.train INFO: Epoch[26] Iteration[340/945] Loss: 1.063, Base Lr: 1.51e-04
2025-04-15 01:16:56,448 transreid.train INFO: Epoch[26] Iteration[680/945] Loss: 1.063, Base Lr: 1.51e-04
2025-04-15 01:17:08,657 transreid.train INFO: Epoch[27] Iteration[340/945] Loss: 1.060, Base Lr: 1.32e-04
2025-04-15 01:17:15,604 transreid.train INFO: Epoch[27] Iteration[680/945] Loss: 1.060, Base Lr: 1.32e-04
2025-04-15 01:17:27,572 transreid.train INFO: Epoch[28] Iteration[340/945] Loss: 1.058, Base Lr: 1.14e-04
2025-04-15 01:17:34,871 transreid.train INFO: Epoch[28] Iteration[680/945] Loss: 1.057, Base Lr: 1.14e-04
2025-04-15 01:17:47,362 transreid.train INFO: Epoch[29] Iteration[340/945] Loss: 1.055, Base Lr: 9.72e-05
2025-04-15 01:17:54,183 transreid.train INFO: Epoch[29] Iteration[680/945] Loss: 1.056, Base Lr: 9.72e-05
2025-04-15 01:18:06,154 transreid.train INFO: Epoch[30] Iteration[340/945] Loss: 1.052, Base Lr: 8.14e-05
2025-04-15 01:18:12,971 transreid.train INFO: Epoch[30] Iteration[680/945] Loss: 1.054, Base Lr: 8.14e-05
2025-04-15 01:18:25,163 transreid.train INFO: Epoch[31] Iteration[340/945] Loss: 1.048, Base Lr: 6.68e-05
2025-04-15 01:18:32,018 transreid.train INFO: Epoch[31] Iteration[680/945] Loss: 1.051, Base Lr: 6.68e-05
2025-04-15 01:18:44,293 transreid.train INFO: Epoch[32] Iteration[340/945] Loss: 1.050, Base Lr: 5.34e-05
2025-04-15 01:18:51,269 transreid.train INFO: Epoch[32] Iteration[680/945] Loss: 1.050, Base Lr: 5.34e-05
2025-04-15 01:19:03,373 transreid.train INFO: Epoch[33] Iteration[340/945] Loss: 1.051, Base Lr: 4.15e-05
2025-04-15 01:19:09,904 transreid.train INFO: Epoch[33] Iteration[680/945] Loss: 1.049, Base Lr: 4.15e-05
2025-04-15 01:19:22,020 transreid.train INFO: Epoch[34] Iteration[340/945] Loss: 1.050, Base Lr: 3.09e-05
2025-04-15 01:19:28,760 transreid.train INFO: Epoch[34] Iteration[680/945] Loss: 1.049, Base Lr: 3.09e-05
2025-04-15 01:19:41,446 transreid.train INFO: Epoch[35] Iteration[340/945] Loss: 1.050, Base Lr: 2.19e-05
2025-04-15 01:19:48,292 transreid.train INFO: Epoch[35] Iteration[680/945] Loss: 1.049, Base Lr: 2.19e-05
2025-04-15 01:20:00,409 transreid.train INFO: Epoch[36] Iteration[340/945] Loss: 1.049, Base Lr: 1.44e-05
2025-04-15 01:20:07,316 transreid.train INFO: Epoch[36] Iteration[680/945] Loss: 1.047, Base Lr: 1.44e-05
2025-04-15 01:20:19,595 transreid.train INFO: Epoch[37] Iteration[340/945] Loss: 1.045, Base Lr: 8.58e-06
2025-04-15 01:20:26,650 transreid.train INFO: Epoch[37] Iteration[680/945] Loss: 1.047, Base Lr: 8.58e-06
2025-04-15 01:20:38,923 transreid.train INFO: Epoch[38] Iteration[340/945] Loss: 1.047, Base Lr: 4.38e-06
2025-04-15 01:20:45,602 transreid.train INFO: Epoch[38] Iteration[680/945] Loss: 1.047, Base Lr: 4.38e-06
2025-04-15 01:20:57,692 transreid.train INFO: Epoch[39] Iteration[340/945] Loss: 1.045, Base Lr: 1.85e-06
2025-04-15 01:21:04,765 transreid.train INFO: Epoch[39] Iteration[680/945] Loss: 1.047, Base Lr: 1.85e-06
2025-04-15 01:21:16,868 transreid.train INFO: Epoch[40] Iteration[340/945] Loss: 1.047, Base Lr: 1.00e-06
2025-04-15 01:21:23,728 transreid.train INFO: Epoch[40] Iteration[680/945] Loss: 1.046, Base Lr: 1.00e-06
2025-04-15 01:21:30,986 transreid.train INFO: Stage1 running time: 0:12:51.105988
2025-04-15 01:21:30,991 transreid.train INFO: start training
2025-04-15 01:21:44,944 transreid.train INFO: Epoch[1] Iteration[10/26] Loss: 20.060, Acc: 0.000, Base Lr: 1.52e-06
2025-04-15 01:21:45,717 transreid.train INFO: Epoch[1] Iteration[20/26] Loss: 17.014, Acc: 0.000, Base Lr: 1.52e-06
2025-04-15 01:21:46,113 transreid.train INFO: Epoch 1 done. Time per batch: 0.097[s] Speed: 328.3[samples/s]
2025-04-15 01:21:47,128 transreid.train INFO: Epoch[2] Iteration[10/26] Loss: 11.094, Acc: 0.000, Base Lr: 2.24e-06
2025-04-15 01:21:47,897 transreid.train INFO: Epoch[2] Iteration[20/26] Loss: 10.847, Acc: 0.000, Base Lr: 2.24e-06
2025-04-15 01:21:48,290 transreid.train INFO: Epoch 2 done. Time per batch: 0.087[s] Speed: 367.6[samples/s]
2025-04-15 01:21:49,327 transreid.train INFO: Epoch[3] Iteration[10/26] Loss: 8.776, Acc: 0.000, Base Lr: 2.96e-06
2025-04-15 01:21:50,079 transreid.train INFO: Epoch[3] Iteration[20/26] Loss: 8.658, Acc: 0.000, Base Lr: 2.96e-06
2025-04-15 01:21:50,498 transreid.train INFO: Epoch 3 done. Time per batch: 0.088[s] Speed: 362.3[samples/s]
2025-04-15 01:21:51,525 transreid.train INFO: Epoch[4] Iteration[10/26] Loss: 8.227, Acc: 0.000, Base Lr: 3.68e-06
2025-04-15 01:21:52,255 transreid.train INFO: Epoch[4] Iteration[20/26] Loss: 8.086, Acc: 0.000, Base Lr: 3.68e-06
2025-04-15 01:21:52,670 transreid.train INFO: Epoch 4 done. Time per batch: 0.087[s] Speed: 368.4[samples/s]
2025-04-15 01:21:53,672 transreid.train INFO: Epoch[5] Iteration[10/26] Loss: 7.550, Acc: 0.006, Base Lr: 4.40e-06
2025-04-15 01:21:54,441 transreid.train INFO: Epoch[5] Iteration[20/26] Loss: 7.549, Acc: 0.009, Base Lr: 4.40e-06
2025-04-15 01:21:54,913 transreid.train INFO: Epoch 5 done. Time per batch: 0.090[s] Speed: 356.6[samples/s]
2025-04-15 01:21:55,945 transreid.train INFO: Epoch[6] Iteration[10/26] Loss: 7.400, Acc: 0.016, Base Lr: 5.12e-06
2025-04-15 01:21:56,723 transreid.train INFO: Epoch[6] Iteration[20/26] Loss: 7.265, Acc: 0.025, Base Lr: 5.12e-06
2025-04-15 01:21:57,116 transreid.train INFO: Epoch 6 done. Time per batch: 0.088[s] Speed: 363.3[samples/s]
2025-04-15 01:21:58,263 transreid.train INFO: Epoch[7] Iteration[10/26] Loss: 6.984, Acc: 0.041, Base Lr: 5.84e-06
2025-04-15 01:21:59,071 transreid.train INFO: Epoch[7] Iteration[20/26] Loss: 6.840, Acc: 0.055, Base Lr: 5.84e-06
2025-04-15 01:21:59,495 transreid.train INFO: Epoch 7 done. Time per batch: 0.095[s] Speed: 336.3[samples/s]
2025-04-15 01:22:00,628 transreid.train INFO: Epoch[8] Iteration[10/26] Loss: 6.526, Acc: 0.116, Base Lr: 6.56e-06
2025-04-15 01:22:01,419 transreid.train INFO: Epoch[8] Iteration[20/26] Loss: 6.374, Acc: 0.169, Base Lr: 6.56e-06
2025-04-15 01:22:01,875 transreid.train INFO: Epoch 8 done. Time per batch: 0.095[s] Speed: 336.1[samples/s]
2025-04-15 01:22:02,920 transreid.train INFO: Epoch[9] Iteration[10/26] Loss: 5.813, Acc: 0.381, Base Lr: 7.28e-06
2025-04-15 01:22:03,716 transreid.train INFO: Epoch[9] Iteration[20/26] Loss: 5.856, Acc: 0.367, Base Lr: 7.28e-06
2025-04-15 01:22:04,149 transreid.train INFO: Epoch 9 done. Time per batch: 0.091[s] Speed: 351.9[samples/s]
2025-04-15 01:22:05,189 transreid.train INFO: Epoch[10] Iteration[10/26] Loss: 5.752, Acc: 0.338, Base Lr: 8.00e-06
2025-04-15 01:22:05,944 transreid.train INFO: Epoch[10] Iteration[20/26] Loss: 5.808, Acc: 0.350, Base Lr: 8.00e-06
2025-04-15 01:22:06,342 transreid.train INFO: Epoch 10 done. Time per batch: 0.088[s] Speed: 364.9[samples/s]
2025-04-15 01:22:07,352 transreid.train INFO: Epoch[11] Iteration[10/26] Loss: 5.099, Acc: 0.553, Base Lr: 8.00e-06
2025-04-15 01:22:08,102 transreid.train INFO: Epoch[11] Iteration[20/26] Loss: 5.068, Acc: 0.570, Base Lr: 8.00e-06
2025-04-15 01:22:08,511 transreid.train INFO: Epoch 11 done. Time per batch: 0.087[s] Speed: 368.9[samples/s]
2025-04-15 01:22:09,546 transreid.train INFO: Epoch[12] Iteration[10/26] Loss: 4.857, Acc: 0.644, Base Lr: 8.00e-06
2025-04-15 01:22:10,333 transreid.train INFO: Epoch[12] Iteration[20/26] Loss: 4.696, Acc: 0.669, Base Lr: 8.00e-06
2025-04-15 01:22:10,768 transreid.train INFO: Epoch 12 done. Time per batch: 0.090[s] Speed: 354.5[samples/s]
2025-04-15 01:22:11,766 transreid.train INFO: Epoch[13] Iteration[10/26] Loss: 4.415, Acc: 0.791, Base Lr: 8.00e-06
2025-04-15 01:22:12,542 transreid.train INFO: Epoch[13] Iteration[20/26] Loss: 4.272, Acc: 0.822, Base Lr: 8.00e-06
2025-04-15 01:22:12,930 transreid.train INFO: Epoch 13 done. Time per batch: 0.086[s] Speed: 370.2[samples/s]
2025-04-15 01:22:13,995 transreid.train INFO: Epoch[14] Iteration[10/26] Loss: 4.037, Acc: 0.869, Base Lr: 8.00e-06
2025-04-15 01:22:14,736 transreid.train INFO: Epoch[14] Iteration[20/26] Loss: 4.058, Acc: 0.881, Base Lr: 8.00e-06
2025-04-15 01:22:15,151 transreid.train INFO: Epoch 14 done. Time per batch: 0.089[s] Speed: 360.1[samples/s]
2025-04-15 01:22:16,227 transreid.train INFO: Epoch[15] Iteration[10/26] Loss: 3.914, Acc: 0.878, Base Lr: 8.00e-06
2025-04-15 01:22:17,002 transreid.train INFO: Epoch[15] Iteration[20/26] Loss: 3.838, Acc: 0.903, Base Lr: 8.00e-06
2025-04-15 01:22:17,412 transreid.train INFO: Epoch 15 done. Time per batch: 0.090[s] Speed: 353.9[samples/s]
2025-04-15 01:22:18,558 transreid.train INFO: Epoch[16] Iteration[10/26] Loss: 3.723, Acc: 0.916, Base Lr: 8.00e-06
2025-04-15 01:22:19,317 transreid.train INFO: Epoch[16] Iteration[20/26] Loss: 3.728, Acc: 0.917, Base Lr: 8.00e-06
2025-04-15 01:22:19,730 transreid.train INFO: Epoch 16 done. Time per batch: 0.093[s] Speed: 345.1[samples/s]
2025-04-15 01:22:21,060 transreid.train INFO: Epoch[17] Iteration[10/26] Loss: 3.637, Acc: 0.953, Base Lr: 8.00e-06
2025-04-15 01:22:21,846 transreid.train INFO: Epoch[17] Iteration[20/26] Loss: 3.657, Acc: 0.947, Base Lr: 8.00e-06
2025-04-15 01:22:22,250 transreid.train INFO: Epoch 17 done. Time per batch: 0.101[s] Speed: 317.6[samples/s]
2025-04-15 01:22:23,257 transreid.train INFO: Epoch[18] Iteration[10/26] Loss: 3.527, Acc: 0.959, Base Lr: 8.00e-06
2025-04-15 01:22:24,077 transreid.train INFO: Epoch[18] Iteration[20/26] Loss: 3.531, Acc: 0.958, Base Lr: 8.00e-06
2025-04-15 01:22:24,515 transreid.train INFO: Epoch 18 done. Time per batch: 0.091[s] Speed: 353.3[samples/s]
2025-04-15 01:22:25,515 transreid.train INFO: Epoch[19] Iteration[10/26] Loss: 3.491, Acc: 0.963, Base Lr: 8.00e-06
2025-04-15 01:22:26,260 transreid.train INFO: Epoch[19] Iteration[20/26] Loss: 3.478, Acc: 0.961, Base Lr: 8.00e-06
2025-04-15 01:22:26,677 transreid.train INFO: Epoch 19 done. Time per batch: 0.086[s] Speed: 370.1[samples/s]
2025-04-15 01:22:27,689 transreid.train INFO: Epoch[20] Iteration[10/26] Loss: 3.395, Acc: 0.975, Base Lr: 8.00e-06
2025-04-15 01:22:28,450 transreid.train INFO: Epoch[20] Iteration[20/26] Loss: 3.407, Acc: 0.975, Base Lr: 8.00e-06
2025-04-15 01:22:28,852 transreid.train INFO: Epoch 20 done. Time per batch: 0.087[s] Speed: 367.9[samples/s]
2025-04-15 01:22:29,898 transreid.train INFO: Epoch[21] Iteration[10/26] Loss: 3.384, Acc: 0.984, Base Lr: 8.00e-06
2025-04-15 01:22:30,667 transreid.train INFO: Epoch[21] Iteration[20/26] Loss: 3.352, Acc: 0.980, Base Lr: 8.00e-06
2025-04-15 01:22:31,084 transreid.train INFO: Epoch 21 done. Time per batch: 0.089[s] Speed: 358.5[samples/s]
2025-04-15 01:22:32,181 transreid.train INFO: Epoch[22] Iteration[10/26] Loss: 3.336, Acc: 0.991, Base Lr: 8.00e-06
2025-04-15 01:22:32,933 transreid.train INFO: Epoch[22] Iteration[20/26] Loss: 3.308, Acc: 0.988, Base Lr: 8.00e-06
2025-04-15 01:22:33,370 transreid.train INFO: Epoch 22 done. Time per batch: 0.091[s] Speed: 349.9[samples/s]
2025-04-15 01:22:34,453 transreid.train INFO: Epoch[23] Iteration[10/26] Loss: 3.275, Acc: 0.988, Base Lr: 8.00e-06
2025-04-15 01:22:35,217 transreid.train INFO: Epoch[23] Iteration[20/26] Loss: 3.261, Acc: 0.989, Base Lr: 8.00e-06
2025-04-15 01:22:35,623 transreid.train INFO: Epoch 23 done. Time per batch: 0.090[s] Speed: 355.1[samples/s]
2025-04-15 01:22:36,668 transreid.train INFO: Epoch[24] Iteration[10/26] Loss: 3.222, Acc: 1.000, Base Lr: 8.00e-06
2025-04-15 01:22:37,456 transreid.train INFO: Epoch[24] Iteration[20/26] Loss: 3.217, Acc: 0.989, Base Lr: 8.00e-06
2025-04-15 01:22:37,867 transreid.train INFO: Epoch 24 done. Time per batch: 0.090[s] Speed: 356.6[samples/s]
2025-04-15 01:22:38,912 transreid.train INFO: Epoch[25] Iteration[10/26] Loss: 3.190, Acc: 0.997, Base Lr: 8.00e-06
2025-04-15 01:22:39,698 transreid.train INFO: Epoch[25] Iteration[20/26] Loss: 3.175, Acc: 0.998, Base Lr: 8.00e-06
2025-04-15 01:22:40,199 transreid.train INFO: Epoch 25 done. Time per batch: 0.090[s] Speed: 356.8[samples/s]
2025-04-15 01:22:41,284 transreid.train INFO: Epoch[26] Iteration[10/26] Loss: 3.158, Acc: 0.997, Base Lr: 8.00e-06
2025-04-15 01:22:42,068 transreid.train INFO: Epoch[26] Iteration[20/26] Loss: 3.179, Acc: 0.992, Base Lr: 8.00e-06
2025-04-15 01:22:42,482 transreid.train INFO: Epoch 26 done. Time per batch: 0.091[s] Speed: 350.6[samples/s]
2025-04-15 01:22:43,599 transreid.train INFO: Epoch[27] Iteration[10/26] Loss: 3.146, Acc: 0.997, Base Lr: 8.00e-06
2025-04-15 01:22:44,323 transreid.train INFO: Epoch[27] Iteration[20/26] Loss: 3.123, Acc: 0.998, Base Lr: 8.00e-06
2025-04-15 01:22:44,736 transreid.train INFO: Epoch 27 done. Time per batch: 0.090[s] Speed: 354.9[samples/s]
2025-04-15 01:22:45,770 transreid.train INFO: Epoch[28] Iteration[10/26] Loss: 3.156, Acc: 1.000, Base Lr: 8.00e-06
2025-04-15 01:22:46,560 transreid.train INFO: Epoch[28] Iteration[20/26] Loss: 3.140, Acc: 1.000, Base Lr: 8.00e-06
2025-04-15 01:22:46,985 transreid.train INFO: Epoch 28 done. Time per batch: 0.090[s] Speed: 355.8[samples/s]
2025-04-15 01:22:48,031 transreid.train INFO: Epoch[29] Iteration[10/26] Loss: 3.100, Acc: 0.997, Base Lr: 8.00e-06
2025-04-15 01:22:48,812 transreid.train INFO: Epoch[29] Iteration[20/26] Loss: 3.090, Acc: 0.998, Base Lr: 8.00e-06
2025-04-15 01:22:49,208 transreid.train INFO: Epoch 29 done. Time per batch: 0.089[s] Speed: 359.9[samples/s]
2025-04-15 01:22:50,292 transreid.train INFO: Epoch[30] Iteration[10/26] Loss: 3.074, Acc: 1.000, Base Lr: 8.00e-07
2025-04-15 01:22:51,048 transreid.train INFO: Epoch[30] Iteration[20/26] Loss: 3.066, Acc: 0.998, Base Lr: 8.00e-07
2025-04-15 01:22:51,460 transreid.train INFO: Epoch 30 done. Time per batch: 0.090[s] Speed: 355.3[samples/s]
2025-04-15 01:22:55,103 transreid.train INFO: Validation Results - Epoch: 30
2025-04-15 01:22:55,103 transreid.train INFO: mAP: 87.83%
2025-04-15 01:22:55,104 transreid.train INFO: CMC curve, Rank-1  :93.64%
2025-04-15 01:22:55,104 transreid.train INFO: CMC curve, Rank-5  :99.09%
2025-04-15 01:22:55,104 transreid.train INFO: CMC curve, Rank-10 :99.09%
2025-04-15 01:22:55,105 transreid.train INFO: Total running time: 0:01:24.112092
